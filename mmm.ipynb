{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_cols = [] \n",
    "for column in train_num.columns:\n",
    "    Q1 = train_num[column].quantile(0.25)\n",
    "    Q3 = train_num[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1 \n",
    "\n",
    "    outliers = (train_num[column] < Q1 - 1.5 * IQR) | (train_num[column] > Q3 + 1.5 * IQR)\n",
    "    \n",
    "    if any(outliers):\n",
    "        outlier_cols.append(column)\n",
    "\n",
    "outlier_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import *\n",
    "\n",
    "fig, axes = plt.subplots(nrows=ceil(len(outlier_cols) / 3), ncols=3, figsize=(15, 5 * ceil(len(outlier_cols) / 3)))\n",
    "\n",
    "for i, column in enumerate(outlier_cols):\n",
    "    row, col = divmod(i, 3)\n",
    "    sns.boxplot(x=train_num[column], ax=axes[row, col], color='skyblue')\n",
    "    axes[row, col].set_title(f'Box Plot for {column}', fontsize=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = test_num.corr()\n",
    "sns.heatmap(correlation, annot=True, cmap='coolwarm')\n",
    "plt.show()\n",
    "plt.figure(figsize=(23, 18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_outliers_cols = ['Credit_Limit', 'Avg_Open_To_Buy', 'Total_Amt_Chng_Q4_Q1',\n",
    "                    'Total_Trans_Amt', 'Total_Ct_Chng_Q4_Q1']\n",
    "\n",
    "def remove_outliers(frame, column):\n",
    "    Q1 = frame[column].quantile(0.25)\n",
    "    Q3 = frame[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "    return frame[(frame[column] >= lower_bound) & (frame[column] <= upper_bound)]\n",
    "\n",
    "for col in large_outliers_cols:\n",
    "    df = remove_outliers(df, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "train_fraud = train[train['target'] == 1]  \n",
    "train_non_fraud = train[train['target'] == 0] \n",
    "\n",
    "\n",
    "train_non_fraud_balanced = resample(train_non_fraud, \n",
    "                                replace=False,    \n",
    "                                n_samples=len(train_fraud), \n",
    "                                random_state=42)   \n",
    "\n",
    "train_balanced = pd.concat([train_fraud, train_non_fraud_balanced])\n",
    "\n",
    "train_balanced = train_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f'Размер сбалансированного набора данных: {train_balanced.shape}')\n",
    "print(train_balanced['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train['target'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coldr = ['session_id','target']\n",
    "X = train_balanced.drop(columns=coldr, axis=1)\n",
    "y = train_balanced['target']\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "numeric_columns_X = X.select_dtypes(include='number').columns\n",
    "X[numeric_columns_X] = scaler.fit_transform(X[numeric_columns_X])\n",
    "\n",
    "test_bebra = test.drop(columns='session_id')\n",
    "numeric_columns_test = test_bebra.select_dtypes(include='number').columns\n",
    "test_bebra[numeric_columns_test] = scaler.transform(test_bebra[numeric_columns_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full, X_temp, y_train_full, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "reg_model.fit(X_train_full, y_train_full)\n",
    "y_pred_proba_reg = reg_model.predict_proba(X_valid)[:, 1]  \n",
    "\n",
    "roc_auc = roc_auc_score(y_valid, y_pred_proba_reg)\n",
    "print(f'ROC-AUC на отложенной выборке: {roc_auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, learning_curve, validation_curve\n",
    "\n",
    "\n",
    "xgb_model = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=xgb_model, \n",
    "                        param_grid=param_grid, \n",
    "                        scoring='roc_auc', \n",
    "                        cv=3, \n",
    "                        verbose=1, \n",
    "                        n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train_full, y_train_full)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(f'Best parameters: {best_params}')\n",
    "\n",
    "best_xgb = grid_search.best_estimator_\n",
    "y_pred_proba_xgb_best = best_xgb.predict_proba(X_valid)[:, 1]\n",
    "roc_auc_xgb_best = roc_auc_score(y_valid, y_pred_proba_xgb_best)\n",
    "print(f'Tuned XGBoost ROC-AUC: {roc_auc_xgb_best}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "feature_importances = best_xgb.feature_importances_\n",
    "features = X.columns\n",
    "sns.barplot(x=feature_importances, y=features)\n",
    "plt.title('Feature Importance for Best XGBoost Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_range = [50, 100, 150]\n",
    "train_scores, test_scores = validation_curve(\n",
    "    XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
    "    X_train_full, y_train_full,\n",
    "    param_name='n_estimators',\n",
    "    param_range=param_range,\n",
    "    scoring='roc_auc',\n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(param_range, train_scores_mean, label=\"Training score\", marker='o')\n",
    "plt.plot(param_range, test_scores_mean, label=\"Validation score\", marker='o')\n",
    "plt.title(\"Validation Curve for XGBClassifier\")\n",
    "plt.xlabel(\"Number of Estimators\")\n",
    "plt.ylabel(\"ROC-AUC\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    best_xgb,\n",
    "    X_train_full, y_train_full,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "    cv=3,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_sizes, train_scores_mean, label=\"Training score\", marker='o')\n",
    "plt.plot(train_sizes, test_scores_mean, label=\"Validation score\", marker='o')\n",
    "plt.title(\"Learning Curve for Best XGBClassifier\")\n",
    "plt.xlabel(\"Training Set Size\")\n",
    "plt.ylabel(\"ROC-AUC\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame({\n",
    "    'session_id': test['session_id'], \n",
    "    'target': y_pred_proba_test\n",
    "})\n",
    "\n",
    "output.to_csv('predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, UploadFile, File\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "with open('xgb_model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "def add_features(data, time_columns, site_columns, time_column_for_year_month):\n",
    "    data['year_month'] = pd.to_datetime(data[time_column_for_year_month], errors='coerce').dt.strftime('%Y%m').astype('float')\n",
    "    data['average_time_gap'] = data[time_columns].apply(\n",
    "        lambda row: pd.Series(pd.to_datetime(row, errors='coerce')).diff().mean().total_seconds()\n",
    "        if row.notnull().all() else None, axis=1\n",
    "    )\n",
    "    data['session_length'] = (pd.to_datetime(data[time_columns[-1]], errors='coerce') - \n",
    "                            pd.to_datetime(data[time_columns[0]], errors='coerce')).dt.total_seconds()\n",
    "    data['unique_sites'] = data[site_columns].nunique(axis=1)\n",
    "    return data\n",
    "\n",
    "@app.post(\"/predict/\")\n",
    "async def predict(file: UploadFile = File(...)):\n",
    "    contents = await file.read()\n",
    "    data = pd.read_csv(StringIO(contents.decode('utf-8')))\n",
    "\n",
    "    time_columns = [col for col in data.columns if col.startswith('time')]\n",
    "    site_columns = [col for col in data.columns if col.startswith('site')]\n",
    "    data = add_features(data, time_columns, site_columns, time_columns[0])\n",
    "\n",
    "    data = data.drop(columns=time_columns)\n",
    "    data = data.drop(columns=['session_id'], axis=1)\n",
    "    predictions = model.predict(data)\n",
    "\n",
    "    return {\"predictions\": predictions.tolist()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import requests\n",
    "st.title(\"Прогнозирование злоумышленника\")\n",
    "st.write(\"Загрузите файл сессий для обработки и анализа.\")\n",
    "\n",
    "uploaded_file = st.file_uploader(\"Загрузите CSV файл\", type=\"csv\")\n",
    "\n",
    "if uploaded_file is not None:\n",
    "    data = uploaded_file.read()\n",
    "    response = requests.post(\"http://127.0.0.1:8000/predict/\", files={\"file\": data})\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        result = pd.DataFrame(response.json())\n",
    "        st.write(\"Обработанные данные и предсказания:\")\n",
    "        st.write(result)\n",
    "\n",
    "        if st.button(\"Сохранить результаты\"):\n",
    "            result.to_csv(\"results.csv\", index=False)\n",
    "            st.success(\"Результаты сохранены в файл results.csv.\")\n",
    "    else:\n",
    "        st.error(\"Ошибка обработки данных.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
